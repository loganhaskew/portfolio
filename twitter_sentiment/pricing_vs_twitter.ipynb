{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1a5a961",
   "metadata": {},
   "source": [
    "## Do Twitter Sentiment and Securities Pricing Show Measurable Correlation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22dfaea",
   "metadata": {},
   "source": [
    "Here's a basic overview of how we will structure our data and step through the analysis.\n",
    "\n",
    "1. Use Tweepy and Twitter API to search tweets for certain keywords within a certain date range.\n",
    "2. Clean the tweets of links, emojis, retweet information, etc. \n",
    "3. Use TextBlob's sentiment analysis (Naive Bayes classifier trained on movie review data) to establish how Twitter 'generally' speaks about the keywords throughout the date range.\n",
    "4. Pull stock, security, or cryptocurrency price over the date range using Yahoo Finance's API.\n",
    "5. Build a dataframe containing relevant price information and sentiment data.\n",
    "6. Use basic plotting and statistics to see how well price and sentiment correlate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b72f0ae",
   "metadata": {},
   "source": [
    "First, let's import the necessary libraries and packages, then set our matplotlib backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1521817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tweepy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "import json\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#!pipreqsnb ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba441122",
   "metadata": {},
   "source": [
    "I have my Twitter API authentication information in a file tucked away. This code just reads the proper key and passes it to Tweepy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47500a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_tokens = []\n",
    "with open('TwitterAPI.txt', 'r') as security:\n",
    "        for i in range(3):\n",
    "            auth_tokens.append(security.readline())\n",
    "            auth_tokens[i] = re.sub('\\\\n', '', auth_tokens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ff8faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    auth = tweepy.OAuth2BearerHandler(auth_tokens[2])\n",
    "    tweepy_api = tweepy.API(auth)\n",
    "    print(\"Success!\")\n",
    "except:\n",
    "    print(\"Authentication Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1a0edb",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef85fca",
   "metadata": {},
   "source": [
    "This cell is home base for building our final dataframe. It prompts input for the keywords, ticker, and date range to be considered. It then calls the functions written below to get tweets, clean them, get stock information, and merge the two dataframes.\n",
    "\n",
    "You will get a bunch of warnings because of the pandas.append deprecation, but the code still functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4645306c",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphrase = input(\"Keyphrase? \")\n",
    "ticker = input(\"What ticker do you think is correlated with the key phrase? \").upper()\n",
    "\n",
    "#Get date range. Use datetime for start date and an int for lookback time.\n",
    "start_date = datetime.datetime.strptime(input(\"Analysis start date (YYYY-MM-DD)? \"), '%Y-%m-%d')\n",
    "lookback_days = int(input(\"How many days do you want to look back (recommended minimum of 15 days)? \"))\n",
    "\n",
    "#set up dataframe for twitter sentiment data.\n",
    "twitter_df = pd.DataFrame(columns=['Date', 'Sentiment'])\n",
    "\n",
    "#loop through date range and get sentiment value for each day. Add the date and sentiment to our twitter_df.\n",
    "for i in range(lookback_days+1):\n",
    "    curr_date_iter = start_date - datetime.timedelta(days=i)\n",
    "    tweet_temp_raw = get_tweets(keyphrase, curr_date_iter)\n",
    "    tweet_holder = make_tweet_list(tweet_temp_raw)\n",
    "    tweet_holder = tweet_prep(tweet_holder)\n",
    "    sentiment_value = get_sentiment(tweet_holder)\n",
    "    twitter_df = twitter_df.append({'Date':curr_date_iter.date(), 'Sentiment':sentiment_value}, ignore_index=True)\n",
    "    \n",
    "twitter_df['Date'] = pd.to_datetime(twitter_df['Date']) \n",
    "twitter_df.set_index('Date', inplace=True)\n",
    "#twitter_df.head()\n",
    "\n",
    "stock_df = get_stock_info(ticker, start_date, lookback_days)\n",
    "#stock_df.head()\n",
    "\n",
    "merged_data = join_dataframes(twitter_df, stock_df)\n",
    "merged_data.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0d92b1",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39845bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(search_phrase, start_time):\n",
    "    \n",
    "    try:\n",
    "        end_time = (start_time + datetime.timedelta(days=1)).astimezone().isoformat()\n",
    "        start_time = start_time.astimezone().isoformat()\n",
    "        \n",
    "        end_time = re.sub('-', '', str((start_time + datetime.timedelta(days=1)).date()) + '0000')\n",
    "        start_time = re.sub('-', '', str(start_time.date()) + '0000')\n",
    "        \n",
    "        raw_tweets = tweepy_api.search_full_archive(label='testAnalysis', query=search_phrase + ' lang:en', \n",
    "                                                    fromDate=start_time, toDate=end_time, maxResults=100)\n",
    "        return raw_tweets\n",
    "    \n",
    "    except:\n",
    "        print(\"Error retrieving tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491cf206",
   "metadata": {},
   "source": [
    "Tweepy returns a ResultSet object. We need to get the JSON data from within and clean everything up. We could also get rid of retweets here, but there is value in keeping them because they can be an indicator of a shared emotion among users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5bcf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tweet_list(raw_tweets):\n",
    "    '''converts returned tweepy ResultSet object into, and returns, list of text elements from the tweets'''\n",
    "    tweet_text = []\n",
    "    for i in range(len(raw_tweets)):\n",
    "        tweet_text.append(raw_tweets[i]._json['text'])\n",
    "    return tweet_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337971d2",
   "metadata": {},
   "source": [
    "Use regular expression library to clean the text from the tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca0a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_prep(tweets):\n",
    "    '''takes list as input and cleans the string elements of links and unwanted characters.\n",
    "    returns lowerscase, cleaned strings\n",
    "    '''\n",
    "    \n",
    "    prepped_tweets = []\n",
    "    for tweet in tweets:\n",
    "        tweet_temp = re.sub(r'@\\S+', \"\", tweet)\n",
    "        tweet_temp = re.sub(r'http\\S+', \"\", tweet_temp)\n",
    "        tweet_temp = re.sub(\"[^a-zA-Z0-9(),\\\"'_ ]\", \"\", tweet_temp)\n",
    "        tweet_temp = re.sub(r'RT  ', \"\", tweet_temp)\n",
    "        tweet_temp = re.sub('  ', ' ', tweet_temp)\n",
    "        \n",
    "        prepped_tweets.append(tweet_temp.lower())\n",
    "        \n",
    "    return prepped_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2960b66",
   "metadata": {},
   "source": [
    "Use TextBlob for sentiment analysis. It really is this simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2061d52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(cleaned_tweets):\n",
    "    '''pass tweets ready for basic sentiment analysis and return an overall \n",
    "    sentiment value from 0 to 1 (0 is negative, 0.5 is neutral, 1 is positive)\n",
    "    '''\n",
    "    \n",
    "    sentiment_scores = np.zeros(len(cleaned_tweets))\n",
    "    \n",
    "    for i, tweet in enumerate(cleaned_tweets):\n",
    "        blobject = TextBlob(tweet)\n",
    "        if blobject.sentiment.polarity > 0:\n",
    "            sentiment_scores[i] = 1\n",
    "        elif blobject.sentiment.polarity < 0:\n",
    "            sentiment_scores[i] = 0\n",
    "        else:\n",
    "            sentiment_scores[i] = 0.5\n",
    "  \n",
    "    return sentiment_scores.sum()/len(sentiment_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdca59c8",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93125d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_info(ticker_string, date, num_days):\n",
    "        '''pass in ticker and date range info. return a dataframe with relevant stock price information'''\n",
    "        \n",
    "        end_date = str((start_date - datetime.timedelta(days=num_days)).date())\n",
    "        stock_info = yf.download(ticker_string, end=str(date.date()), start=end_date, \n",
    "                                 index_as_date=True, interval='1d').drop(columns=['High', 'Low', 'Adj Close'])\n",
    "        stock_info['Day Change %'] = 100 * (stock_info['Close']-stock_info['Open'])/stock_info['Open']\n",
    "        \n",
    "        stock_info.reset_index(inplace=True)\n",
    "        stock_info['Date'] = stock_info['Date'].apply(lambda x: x.date())\n",
    "        stock_info['Date'] = pd.to_datetime(stock_info['Date'])\n",
    "        stock_info.set_index('Date', inplace=True)\n",
    "            \n",
    "        return stock_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4075082",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033a45dd",
   "metadata": {},
   "source": [
    "Join our twitter and stock dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe6f1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_dataframes(twitter, stock):\n",
    "    '''join the twitter sentiment and stock price history dataframe on date'''\n",
    "    twitter.reset_index(inplace=True)\n",
    "    stock.reset_index(inplace=True)\n",
    "    merged_data = pd.merge(twitter, stock, how='outer', on='Date').set_index('Date')\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd4306a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cef8fb2",
   "metadata": {},
   "source": [
    "Now that we have our dataframes. Let's start doing some EDA. Let's plot a few different things to see if there is anything obvious that pops out. After that we can quantify correlation and try to see if there is anything statiscally significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878664f8",
   "metadata": {},
   "source": [
    "I think I'll just put in pictures for all of these so we don't overwrite anything next time the code is run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17306b4",
   "metadata": {},
   "source": [
    "This cell here is just to to check out the data and debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45386f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_data.dropna(inplace=True)\n",
    "#merged_data.reset_index(inplace=True)\n",
    "#merged_data\n",
    "print(merged_data.dtypes) #note that sentiment is dtype object\n",
    "print(merged_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07f7a78",
   "metadata": {},
   "source": [
    "I first tried a small test with Tesla and there was not exactly a lot to go on. We could almost fit an ellipse to the data... but I'll put the image here just for fun. Maybe I'll try something like Bitcoin. It might be more susceptible to internet's feelings.\n",
    "\n",
    "<img src=\"tesla_test.png\" align=\"left\" width=400 height=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761c1b98",
   "metadata": {},
   "source": [
    "Well, Bitcoin didn't give us much more to go on. But for the sake of completion, let's continue anyway.\n",
    "\n",
    "<img src=\"btc_scatter.png\" align=\"left\" width=400 height=400/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdaed5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = merged_data['Sentiment'].astype(float)\n",
    "y = merged_data['Day Change %'].astype(float)\n",
    "m, b = np.polyfit(x, y, 1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter('Sentiment', 'Day Change %', data=merged_data, label='')\n",
    "ax.plot(x, m*x+b, color='red', label='y={:.2f}x+{:.2f}'.format(m, b))\n",
    "ax.set(xlabel='Sentiment', ylabel='Stock % Change', title='BTC Price vs Twitter Sentiment')\n",
    "fig.set_dpi(100)\n",
    "plt.legend(fontsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91057629",
   "metadata": {},
   "source": [
    "<img src=\"line_fit.png\" align=\"left\" width=400 height=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd443029",
   "metadata": {},
   "source": [
    "Let's quantify the correlation... Or, more correctly, lack thereof. \n",
    "\n",
    "And just to get a better feel for the data, let's plot the sentiment and day change percentage histograms as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d77322",
   "metadata": {},
   "source": [
    "Numpy returns the correlation coefficient matrix. We can simplify by accessing one of the relevant elements since we only have two variables to consider here. For the Bitcoin test case, it was 0.031."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f275194",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = np.corrcoef(x, y)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6256754",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, ax2 = plt.subplots()\n",
    "ax2.hist('Sentiment', data=merged_data)\n",
    "fig2.set_dpi(100)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d46dcd",
   "metadata": {},
   "source": [
    "<img src=\"sent_hist.png\" align=\"left\" width=400 height=400/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d6dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3, ax3 = plt.subplots()\n",
    "ax3.hist('Day Change %', data=merged_data)\n",
    "fig3.set_dpi(100)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8e03ac",
   "metadata": {},
   "source": [
    "<img src=\"daychange_hist.png\" align=\"left\" width=400 height=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea7da84",
   "metadata": {},
   "source": [
    "So we actually have something resembling normal distributions (albeit with some holes and skew) which means our correlation calculation is probably trustworthy when it tells us sentiment and change in securities price are not well correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5340ff",
   "metadata": {},
   "source": [
    "The final thing I'd like to do to round this out is find the $R^2$ value. We might as well pull in something from scikit-learn and do our due diligence. Sure, we could easily do this within numpy, but that's boring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8090af09",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2= r2_score(y, m*x+b)\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d15825",
   "metadata": {},
   "source": [
    "And sometimes boring is good to at least to check our work. We come up with the same answer here. So it's safe to say this model is junk (which we knew from the beginning). Almost no variance can be predicted by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c727de",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSr=np.sum(np.square(merged_data['Day Change %'] - (m*merged_data['Sentiment'] + b)))\n",
    "SSt=np.sum(np.square(merged_data['Day Change %'] - np.mean(merged_data['Day Change %'])))\n",
    "r2_np = 1 - (SSr/SSt)\n",
    "r2_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d80bce",
   "metadata": {},
   "source": [
    "There are quite a few ways to improve from where we are so this list is definitely not exhaustive.\n",
    "\n",
    "First, I'd like to use a different Twitter endpoint to allow for more data. Unfortunately, we can only make 50 calls a month (each returning up to 100 tweets) for the full twitter archive. We can get a bit more freedom once we get within more recent timeframes though.\n",
    "\n",
    "A better option (if we were to run this again) would be getting more granular with respect to time rather than aggregating and looking at one day at a time. \n",
    "\n",
    "Our TextBlob model is also not optimal for what we are doing. It's a Naive Bayes classifier that was trained on movie reviews. Training our own model would certainly be a better option here.\n",
    "\n",
    "As I'm writing this, I forgot pandas has datetime functionality and I worked through everything with the datetime package... Live and learn. \n",
    "\n",
    "The goal here wasn't to walk away with some revelation we never saw coming. This is really just practice for different ways to interact with data or even use other resources to supplement data we may already have. We have an almost non-existent data set here, but it was fun to put a lot of moving parts together."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
